<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<!-- Copyright 1996 Emergent Technologies Inc. -->
<!-- All rights reserved. -->
<html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"><title>Emergent Technologies Inc. -- LISP Machine Progress Report</title><meta name="description" content="Lisp machine progress report"><meta name="keywords" content="LISP, hardware"><style type="text/css">
<!--
BODY {
    font-family: trebuchet MS;
}
    P {
        text-indent: .25in;
      }
CODE, PRE {
        font-family: courier new;
    }
-->
</style></head>

<body background="memo444_files/index.html"><table width="100%"><tbody><tr><td>Memo No. 444</td><td><center><h2>MASSACHUSETTS INSTITUTE OF TECHNOLOGY<br>ARTIFICIAL INTELLIGENCE LABORATORY</h2></center></td><td><div align="right">August 1977</div></td></tr></tbody></table><center><h1>LISP Machine Progress Report</h1><h2>by the Lisp Machine Group</h2><h3>Alan Bawden<br>Richard Greenblatt<br>Jack Holloway<br>Thomas Knight<br>David Moon<br>Daniel Weinreb</h3></center><blockquote><i>Abstract</i>

<p>This informal paper introduces the Lisp machine, describes the
goals and current status of the project, and explicates some of the
key ideas.  It covers the Lisp machine implementation, Lisp as a
system language, input/output, representation of data, representation
of programs, control structures, storage organization, garbage
collection, the editor, and the current status of the work.</p>

<p>This report describes research done at the Artificial Intelligence
laboratory of the Massachusetts Institute of Technology.  Support for
the laboratory's artificial intelligence research is provided in part
by the Advanced Research Projects Agency of the Department of Defense
under Office of Naval Research contract
N00014-75-C-0643.</p></blockquote>

<h2>INTRODUCTION</h2>

<p>The LISP Machine is a new computer system designed to provide a
high performance and economical implementation of the LISP programming
language.</p>

<p>The LISP language is used widely in the artificial intelligence
research community, and is rapidly gaining adherents outside this
group.  Most serious Lisp usage has historically been an the DEC
PDP-10 computer, and both "major"' implementations (Interlisp at
BBN/XEROX and Maclisp at M.I.T.) were originally done on the PDP-10.
Our personal experience has largely been with the Maclisp dialect of
LISP, which was originally written in 1965.</p>

<p>Over the years, dramatic changes have taken place in the Maclisp
implementation.  At a certain point, however, modification and
reimplementation of a language on a given machine can no longer
efficiently gloss over basic problems in the architecture of the
computer system.  We, and many others, believe this is now the case on
the PDP-10 and similar time-shared computer systems.</p>

<p>Time sharing was introduced when it became apparent that computers
are easier to use in an interactive fashion than in a batch system,
and that during an interactive session a user typically uses only a
small fraction of the processor and memory available; often during
much of time his process is idle or waiting, and so the computer can
be multiplexed among many users while giving each the impression that
he is on his own machine.</p>

<p>However, in the Lisp community there has been a strong trend
towards programs which are very highly interactive, very large, and
use a good deal of computer time; such programs include advanced
editors and debuggers, the MACSYMA system, and various programming
assistants. When running programs such as these, which spend very
significant amounts of time supporting user interactions, time sharing
systems such as the PDP-10 run into increased difficulties.  Not only
is the processor incapable of providing either reasonable throughput
or adequate response time for a reasonable number of users, but the
competition for main memory results in large amounts of time being
spent swapping pages in and out (a condition known as "thrashing").
Larger and larger processors and memory, and more and more complex
operating systems, are required, with more than proportionally higher
cost, and still the competition for memory remains a bottleneck.  The
programs are sufficiently large, and the interactions sufficiently
frequent, that the usual time-sharing strategy of swapping the program
out of memory while waiting for the user to interact, then swapping it
back in when the user types something cannot be successful because the
swapping cannot happen fast enough.</p>

<p>The Lisp Machine is a personal computer. Personal computing means
that the processor and main memory are not time-division multiplexed,
instead each person gets his own.  The personal computation system
consists of a pool of processors, each with its own main memory, and
its own disk for snapping. When a user logs in, he is assigned a
processor, and he has exclusive use of it for the duration of the
session. When he logs out, the processor is returned to the pool for
the next person to use. This way, there is no competition from other
users for memory; the pages the user is frequently referring to remain
in core, and so swapping overhead is considerably reduced. Thus the
Lisp machine solves a basic problem of time sharing Lisp systems.</p>

<p>The user also gets a much higher degree of service from a Lisp
machine than from a timesharing system, because he can use the full
throughput capacity of the processor and the disk. Although these are
quite inexpensive compared to those used in PDP-10 timesharing
systems, they are comparable in speed.  In fact, since disk access
times are mainly limited by physical considerations, it often turns
out that the disk used in a personal computer system is less expensive
simply because of its smaller size, and has fully comparable
throughput characteristics to the larger disk used by a timesharing
system.</p>

<p>In a single-user machine, there is no penalty for interactiveness,
since there are no competing users to steal a program's memory while
it is waiting for its user to type. Thus the Lisp machine system,
unlike time sharing systems, encourages highly interactive programs.
It puts service to the user entirely ahead of efficiency
considerations.</p>

<p>Another problem with the PDP-10 LISP implementation is the small
address space of the PDP-10 processor. Many Lisp systems, such as
MACSYMA and Woods's LUNAR program, have difficulty running in an
18-bit address space.  This problem is further compounded by the
inefficiency of the information coding of compiled Lisp code;
compilers for the PDP-10 produce only a limited subset of the large
instruction set made available by the hardware, and usually make
inefficient use of the addressing modes and fields provided. It is
possible to design much more compact instruction sets for Lisp code.
Future programs are likely to be quite a bit bigger; intelligent
systems with natural language front ends may well be five or ten times
the size of a PDP-10 address space.</p>

<p>The Lisp Machine has a 24 bit virtual address space and a compact
instruction set, described later in this paper. Thus much larger
programs may be used, without running into address space limitations.
Since the instruction set is designed specifically for the Lisp
language, the compiler is much simpler than the PDP-10 compiler,
providing faster and more reliable compilation.</p>

<p>The Lisp machine's compact size and simple hardware construction
are likely to make it more reliable than other machines, such as the
PDP-10; the prototype machine has had almost no hardware failures.</p>

<p>Much of the inspiration for the Lisp Machine, project comes from
the pioneering research into personal computing and display-oriented
systems done by Xerox's Palo Alto Research Canter.</p>


<h2>THE LISP MACHINE IMPLEMENTATION:</h2>

<p>Each logged in user of the Lisp Machine system has a processor, a
memory, a keyboard, a display, and a means of getting to the shared
resources.  Terminals, of course, are placed in offices and various
rooms; ideally there would be one in every office. The processors,
however, are all kept off in a machine room.  Since they may need
special environmental conditions, and often make noise and take up
space, they are not welcome office companions. The number of
processors is unrelated to the number of terminals, and may be smaller
depending on economic circumstance.</p>

<p>The processor is implemented with a microprogrammed architecture.
It is called the CONS machine, designed by Tom Knight [CONS].  CONS is
a very unspecialized machine with 32-bit data paths and 24-bit address
paths.  It has a large microcode memory (16k of 48-bit words) to
accommodate the large amount of specialized microcode to support Lisp.
It has hardware for extracting and depositing arbitrary fields in
arbitrary registers, which substitutes for the specialized data paths
found in conventional microprocessors. It does not have a cache, but
does have a "pdl buffer" (a memory with hardware push-down pointer)
which acts as a kind of cache for the stack, which is where most of
the memory references go in Lisp.</p>

<p>Using a very unspecialized processor was found to be a Good idea
for several reasons.  For one thing, it is faster, less expensive, and
easier to debug.  For another thing, it is much easier to
microprogram, which allows us to write and debug the large amounts of
microcode required to support a sophisticated Lisp system with high
efficiency. It also makes feasible a compiler which generates
microcode, allowing users to microcompile some of their functions to
increase performance.</p>

<p>The memory is typically 64K of core or semiconductor memory, and is
expandable to about 1 million words. The full virtual address space is
stored an a 16 million word disk and paged into core (or
semiconductor) memory as required.  A given virtual address is always
located at the same place an the disk.  The access time of the core
memory is about 1 microsecond, and of the disk about 25 milliseconds.
Additionally, there is an internal 1K buffer used for holding the top
of the stack (the PDL buffer) with a 200ns access time (see [CONS] for
more detail).</p>

<p>The display is a raster scan TV driven by a 1/4 Mbit memory,
similar to the TV display system now in use on the Artificial
Intelligence Lab's PDP-10, Characters are drawn entirely by software,
and so any type or size of font can be used, including variable width
and several styles at the same time.  One of the advantages of having
an unspecialized microinstruction processor such as CONS is that one
can implement a flexible terminal in software for less cost than an
inflexible, hardwired conventional terminal. The TV system is easily
expanded to support gray scale, high resolution, and color. This
system has been shown to be very useful for both character display and
graphics.</p>

<p>The keyboard is the same type as is used on the Artificial
Intelligence Lab TV display system; it has several levels of
control/shifting to facilitate easy single-keystroke commands to
programs such as the editor.  The keyboard is also equipped with a
speaker for beeping, and a pointing device, usually a mouse
[MOUSE].</p>

<p>The shared resources are accessed through a 10 million bit/sec
packet switching network with completely distributed control. The
shared resources are to include a highly reliable file system
implemented on a dedicated computer equipped with state of the art
disks and tapes, specialized I/O devices such as high-quality hardcopy
output, special-purpose processors, and connections to the outside
world (e.g. other computers in the building, and the ARPANET).</p>

<p>As in a time sharing system, the file system is shared between
users.  Time sharing has pointed up many advantages of a shared file
system, such as common access to files, easy inter-user communication,
centralized program maintenance, centralized backup, etc. There are no
personal disk packs to be lost, dropped by users who are not competent
as operators, or to be filled with copies of old, superseded
software.</p>

<p>The complete LISP machine, including processor, memory, disk,
terminal, and connection to the shared file system, is packaged in a
single 19" logic cabinet, except for the disk which is freestanding.
The complete machine would be likely to cost about $80,000 if
commercially produced.  Since this is a complete, fully-capable system
(for one user at a time) it can substantially lower the cost of entry
by new organizations into serious Artificial Intelligence work.</p>


<h2>LISP AS A SYSTEM LANGUAGE:</h2>

<p>In the software of the Lisp Machine system, code is written in only
two languages (or "levels"): Lisp, and CONS machine microcode There is
never any reason to hand-code macrocode, since it corresponds so
closely with Lisp; anything one could write in macrocode could be more
easily end clearly written in the corresponding Lisp.  The
<code>READ</code>, <code>EVAL</code>, and <code>PRINT</code> functions
are completely written in Lisp, including their subfunctions (except
that <code>APPLY</code> of compiled functions is in micro-code).  This
illustrates the ability to write system functions in Lisp.</p>

<p>In order to allow various low-level operations to be performed by
Lisp code, a set of "sub-primitive" functions exist. Their names by
convention begin with a "%", so as to point out that they are capable
of performing unLispy operations which may result in meaningless
pointers. These functions provide "machine level" capabilities, such
as performing byte-deposits into memory.  The compiler converts calls
to these sub-primitives into single instructions rather than
subroutine calls.  Thus Lisp-coded low-level operations are just as
efficient as they would be in machine language on a conventional
machine.</p>

<p>In addition to sub-primitives, the ability to do system programming
in Lisp depends on the Lisp machine's augmented array feature.  There
are several types of arrays, one of which is used to implement
character strings. This makes it easy and efficient to manipulate
strings either as a whole or character by character.  An array can
have a "leader", which is a little vector of extra information tacked
on.  The leader always contains Lisp objects while the array often
contains characters or small packed numbers. The leader facilitates
the use of arrays to represent various kinds of abstract object types.
The presence in the language of both arrays and lists gives the
programmer more control over data representation.</p>

<p>A traditional weakness of Lisp has been that functions have to take
a fixed number of arguments.  Various implementations have added
kludges to allow variable numbers of arguments; these, however, tend
either to slow down the function-calling mechanism, even when the
feature is not used, or to force peculiar programming styles.
Lisp-machine Lisp allows functions to have optional parameters with
automatic user-controlled defaulting to an arbitrary expression in the
case where a corresponding argument is not supplied.  It is also
possible to have a "rest" parameter, which is bound to a list of the
arguments not bound to previous parameters. This is frequently
important to simplify system programs and their interfaces.</p>

<p>A similar problem with Lisp function calling occurs when one wants
to return more than one value.  Traditionally one either returns a
list or stores some of the values into global variables.  In Lisp
machine Lisp, there is a multiple-value-return feature which allows
multiple values to be returned without going through either of the
above subterfuges.</p>

<p>Lisp's functional orientation and encouragement of a programming
style of small modules and uniform date structuring is appropriate for
good system programming. The Lisp machine's micro-coded subroutine
calling mechanism allows it to also be efficient.</p>

<p>Paging is handled entirely by the microcode, and is considered to
be at a very low level (lower level than any kind of scheduling).
Making the guts of the virtual memory invisible to all Lisp code and
most microcode helps keep things simple.  It would not be practical in
a time sharing system, but in a one-user machine it is reasonable to
put paging at the lowest level and forget about it, accepting the fact
that sometimes the machine will be tied up waiting for the disk and
unable to run any Lisp code.</p>

<p>Micro-coded functions can be called by Lisp code by the usual Lisp
calling mechanism, and provision is made for micro-coded functions to
call macro-coded functions. Thus there is a uniform calling convention
throughout the entire system.  This has the effect that uniform
subroutine packages can be written, (for example the TV package, or
the EDITOR package) which can be called by any other program.  (A
similar capability is provided by Multics, but not by ITS nor
TENEX).</p>

<p>Many of the capabilities which systems programmers write over and
over again in an ad hoc way are built into the Lisp language, and are
sufficiently good in their Lisp-provided form that it usually is not
necessary to waste time worrying about how to implement better ones.
These include symbol tables, storage management, both fixed and
flexible data structures, function-calling, and an interactive user
interface.</p>

<p>Our experience has been that we can design, code, and debug new
features much faster in Lisp-machine programs than in PDP-10 programs,
whether they are written in assembler language or in traditional
"higher-level languages.</p>

<h2>INPUT/OUTPUT:</h2>

<h3>Low level:</h3>

<p>The Lisp machine processor (CONS) has two busses used for accessing
external devices: the "XBUS", and the "UNIBUS". The XBUS is 32 bits
wide, and is used for the disk and for main memory. The UNIBUS is a
standard PDP-11 16-bit bus, used for various I/O devices.  It allows
commonly available PDP-11 compatible devices to be easily attached to
the Lisp machine.</p>

<p>Input/output software is essentially all written in Lisp; the only
functions provided by the microcode are <code>%UNIBUS-READ</code> and
<code>%UNIBUS-WRITE</code>, which know the offset of the UNIBUS in
physical address space, and refer to the corresponding location.  The
only real reason to have these in microcode is to avoid a timing error
which can happen with some devices which have side effects when read.
It is Lisp programs, not special microcode, which know the location
and function of the registers in the keyboard, mouse, TV, and cable
network interfaces.  This makes the low-level I/O code just as
flexible and easy to modify as the high level code.</p>

<p>There are also a couple of microcoded routines which speed up the
drawing of characters in the TV memory. These do not do anything which
could not be done in Lisp, but they are carefully hand-coded in
microcode because we draw an awful lot of characters.</p>

<h3>High level:</h3>

<p>Many programs perform simple stream-oriented
(sequential characters) I/O.  In order that these programs be kept
device-independent, there is a standard definition of a "stream": a
stream is a functional object which takes one required argument and
one optional argument. The first argument is a symbol which is a
"command" to the stream, such as "<code>TYI</code>", which means
"input one character, and return it" and "<code>TYO</code>", which
means "output one character".  The character argument to the
<code>TYO</code> command is passed in the second argument to the
stream. There are several other standard optional stream operations,
for several purposes including higher efficiency. In addition
particular devices can define additional operations for their own
purposes.</p>

<p>Streams can be used for I/O to files in the file system, strings
inside the Lisp Machine, the terminal, editor buffers, or anything
else which is naturally represented as sequential characters.</p>

<p>For I/O which is of necessity device-dependent, such as the
sophisticated operations performed on the TV by the editor, which
include multiple blinkers and random access to the screen, special
packages of Lisp functions are provided, and there is no attempt to be
device-independent. (See documentation an the TV and network
packages).</p>

<p>In general, we feel no regret at abandoning device independence in
interactive programs which know they are using the display. The
advantages to be gained from sophisticated high-bandwidth
display-based interaction far outweigh the advantages of
device-independence. This does mean that the Lisp machine is really
not usable from other than its own terminal; in particular, it cannot
be used remotely over the ARPANET.</p>

<h2>REPRESENTATION OF DATA:</h2>

<p>A Lisp object in Maclisp or Interlisp is represented as an 18 bit
pointer, and the datatype of the object is determined from the
pointer; each page of memory can only contain objects of a single
type.  In the Lisp machine, Lisp objects are represented by a 5 bit
datatype field, and a 24 bit pointer.  (The Lisp machine virtual
address space is 24 bits). There are a variety of datatypes (most of
the 32 possible codes are now in use), which have symbolic names such
as <code>DTP-LIST</code>, <code>DTP-SYMBOL</code>,
<code>DTP-FIXNUM</code>, etc.</p>

<p>The Lisp machine data types are designed according to these
criteria: there should be a wide variety of useful and flexible data
types.  Some effort should be made to increase the bit-efficiency of
data representation, in order to improve performance. The programmer
should be able to exercise control over the storage and representation
of data, if he wishes. It must always be possible to take an anonymous
piece of data and discover its type; this facilitates storage
management. There should be as much type-checking and error-checking
as feasible in the system.</p>

<p>Symbols are stored as four consecutive words, each of which
contains one object.  The words are termed the PRINT-NAME cell, the
VALUE cell, the FUNCTION cell, and the PROPERTY LIST cell. The PRINT
NAME cell holds a string object, which is the printed representation
of the symbol. The PROPERTY LIST cell, of course, contains the
property list, and the VALUE CELL contains the current value of the
symbol (it is a shallow-binding system). The FUNCTION cell replaces
the task of the EXPR, SUBR, FEXPR, MACRO, etc. properties in Maclisp.
When a form such as <code>(FOO ARG1 ARG2)</code> is evaluated, the
object in <code>FOO</code>'s function cell is applied to the
arguments.  A symbol object has datatype <code>DTP-SYMBOL</code>, and
the pointer is the address of these four words.</p>

<p>Storage of list structure is somewhat more complicated. Normally a
"list object" has datatype <code>DTP-LIST</code>, and the pointer is
the address of a two word block; the first word contains the
<code>CAR</code>, and the second the <code>CDR</code> of the node.</p>

<p>However, note that since a Lisp object is only 29 bits (24 bits of
pointer and 5 bits of data-type), there are three remaining bits in
each word.  Two of these bits are termed the CDR-CODE field, and are
used to compress the storage requirement of list structure. The four
possible values of the CDR-CODE field are given the symbolic names
<code>CDR-NORMAL</code>, <code>CDR-ERROR</code>,
<code>CDR-NEXT</code>, and <code>CDR-NIL</code>.
<code>CDR-NORMAL</code> indicates the two-word block described above.
<code>CDR-NEXT</code> and <code>CDR-NIL</code> are used to represent a
list as a vector, taking only half as much storage as usual; only the
<code>CAR</code>s are stored. The <code>CDR</code> of each location is
simply the next location, except for the last, whose <code>CDR</code>
is <code>NIL</code>. The primitive functions which create lists
(<code>LIST</code>, <code>APPEND</code>, etc.) create these compressed
lists.  If <code>RPLACD</code> is done on such a list, it is
automatically changed back to the conventional two-word
representation, in a transparent way.</p>

<p>The idea is that in the first word of a list node the
<code>CAR</code> is represented by 29 bits, and the <code>CDR</code>
is represented by 2 bits. It is a compressed pointer which can take on
only 3 legal values: to the symbol <code>NIL</code>, to the next
location after the one it appears in, or indirect through the next
location.<code>CDR-ERROR</code> is used for words whose address should
not aver be in a list object; in a "full node", the first word is
<code>CDR-NORMAL</code>, and the second is <code>CDR-ERROR</code>.  It
is important to note that the cdr-code portion of a word is used in a
different way from the data-type and pointer portion; it is a property
of the memory cell itself, not of the cell's contents. A "list object"
which is represented in compressed form still has data type
<code>DTP-LIST</code>, but the cdr code of the word addressed by its
pointer field is <code>CDR-NEXT</code> or <code>CDR-NIL</code> rather
than <code>CDR-NORMAL</code>.</p>

<p>Number objects may have any of three datatypes. "FIXNUMs", which
are 24-bit signed integers, are represented by objects of datatype
<code>DTP-FIX</code>, whose "pointer" parts are actually the value of
the number. Thus fixnums, unlike all other objects, do not require any
"CONS"ed storage for their representation.  This speeds up arithmetic
programs when the numbers they work with are reasonably small.  Other
types of numbers, such as floating point, BIGNUMs (integers of
arbitrarily high precision), complex numbers, and so on, are
represented by objects of datatype <code>DTP-EXTENDED-NUMBER</code>
which point to a block of storage containing the details of the
number.  The microcode automatically converts between the different
number representations as necessary, without the need for explicit
declarations on the programmer's part.</p>

<p>There is also a datatype <code>DTP-PDL-NUMBER</code>, which is
almost the same as <code>DTP-EXTENDED-NUMBER</code>.  The difference
is that pdl numbers can only exist in the pdl buffer (a memory
internal to the machine which holds the most recent stack frames), and
their blocks of storage are allocated in a special area.  Whenever an
object is stored into memory, if it is a pdl number its block of
storage is copied, and an ordinary extended number is substituted. The
idea of this is to prevent intermediate numeric results from using up
storage and causing increased need for garbage collection. When the
special pdl number area becomes full, all pdl numbers can quickly be
found by scanning the pdl buffer. Once they have been copied out into
ordinary numbers, the special area is guaranteed empty and can be
reclaimed, with no need to garbage collect nor to look at other parts
of memory.  Note that these are not at all the same as pdl numbers in
Maclisp; however, they both exist for the same reason.  </p>

<p>The most important other data type is the array. Some problems are
best attacked using data structures organized in the list-processing
style of Lisp, and some are best attacked using the array-processing
style of FORTRAN.  The complete programming system needs both. As
mentioned above, Lisp Machine arrays are augmented beyond traditional
Lisp arrays in several ways.  First of all, we have the ordinary
arrays of Lisp objects, with one or more dimensions.  Compact storage
of positive integers, which may represent characters or other
non-numeric entities, is afforded by arrays of 1-bit, 2-bit, 4-bit,
8-bit, or 16-bit elements.</p>

<p>For string-processing, there are string-arrays, which are usually
one-dimensional and have 8-bit characters as elements.  At the
microcode level strings are treated the same as 8-bit arrays, however
strings are treated differently by <code>READ</code>,
<code>PRINT</code>, <code>EVAL</code>, and many other system and user
functions.  For example, they print out as a sequence of characters
enclosed in quotes.  The characters in a character string can be
accessed and modified with the same array-referencing functions as one
uses for any other type of array.  Unlike arrays in other Lisp
systems, Lisp machine arrays usually have only a single word of
overhead, so the character strings are quite storage-efficient.</p>

<p>There are a number of specialized types of arrays which are used to
implement other data types, such as stack groups, internal system
tables, and, most importantly, the refresh memory of the TV display as
a two-dimensional array of bits.</p>

<p>An important additional feature of Lisp machine arrays is called
"array leaders."  A leader is a vector of Lisp objects, of
user-specified size, which may be tacked on to an array. Leaders are a
good place to remember miscellaneous extra information associated with
an array.  Many data structures consist of a combination of an array
and a record (see below); the array contains a number of objects all
of the same conceptual type, while the record contains miscellaneous
items all of different conceptual types. By storing the record in the
leader of the array, the single conceptual data structure is
represented by a single actual object.  Many data structures in
Lisp-machine system programs work this way.</p>

<p>Another thing that leaders are used for is remembering the current
length of a partially-populated array.  By convention, array leader
element number 0 is always used for this.  </p>

<p>Many programs use data objects structured as "records" that is, a
compound object consisting of a fixed number of named sub-objects. To
facilitate the use of records, the Lisp machine system includes a
standard set of macros for defining, creating, and accessing record
structures. The user can choose whether the actual representation is
to be a Lisp list, an array, or an array-leader.  Because this is done
with macros, which translate record operations into the lower-level
operations of basic Lisp, no other part of the system needs to know
about records.</p>

<p>Since the reader and printer are written in Lisp and
user-modifiable, this record-structure feature could easily be
expanded into a full-fledged user-defined data type facility by
modifying read and print to support input and output of record
types.</p>

<p>Another data type is the "locative pointer". This is an actual
pointer to a memory location, used by low-level system programs which
need to deal with the guts of data representation.  Taking
<code>CAR</code> or <code>CDR</code> of a locative gets the contents
of the pointed-to location, and <code>RPLACA</code> or
<code>RPLACD</code> stores. It is possible to LAMBDA-bind the
location.  Because of the tagged architecture and highly-organized
storage, it is possible to have a locative pointer into the middle of
almost anything without causing trouble with the garbage
collector.</p>

<h2>REPRESENTATION OF PROGRAMS:</h2>

<p>In the Lisp Machine there are three representations for programs.
Interpreted Lisp code is the slowest, but the easiest for programs to
understand and modify.  It can be used for functions which are being
debugged, for functions which need to be understood by other
functions, and for functions which are not worth the bother of
compiling. A few functions, notably <code>EVAL</code>, will not work
interpreted.</p>

<p>Compiled Lisp ("macrocode") is the main representation for
programs.  This consists of instructions in a somewhat conventional
machine language, whose unusual features will be described below.
Unlike the case in many other Lisp systems, macrocode programs still
have full checking for unbound variables, data type errors, wrong
number of arguments to a function, and so forth, so it is not
necessary to resort to interpreted code just to get extra checking to
detect bugs.  Often, after typing in a function to the editor, one
skips the interpretation step and requests the editor to call the
compiler on it, which only takes a few seconds since the compiler is
always in the machine and only has to be paged in.</p>

<p>Compiled code on the Lisp Machine is stored inside objects called
(for historical reasons) Function Entry Frames (FEFs).  For each
function compiled, one FEF is created, and an object of type
<code>DTP-FEF-POINTER</code> is stored. in the function cell of the
symbol which is the name of the function. A FEF consists of some
header information, a description of the arguments accepted by the
function, pointers to external Lisp objects needed by the function
(such as constants and special variables), and the macrocode which
implements the function.</p>

<p>The third form of program representation is microcode.  The system
includes a good deal of hand-coded microcode which executes the
macrocode instructions, implements the data types and the
function-calling mechanism, maintains the paged virtual memory, does
storage allocation and garbage collection, and performs similar
systemic functions. The primitive operations on the basic data types,
that is, <code>CAR</code> and <code>CDR</code> for lists, arithmetic
for numbers, reference and store for arrays, etc.  are implemented as
microcode subroutines.  In addition, a number of commonly-used Lisp
functions, for instance <code>GET</code> and <code>ASSQ</code>, are
hand-coded in microcode for speed.</p>

<p>In addition to this system supplied microcode, there is a feature
called micro compilation.  Because of the simplicity and generality of
the CONS microprocessor, it is feasible to write a compiler to compile
user-written Lisp functions directly into microcode, eliminating the
overhead of fetching and interpreting macroinstructions.  This can be
used to boost performance by microcompiling the most critical routines
of a program. Because it is done by a compiler rather than a system
programmer, this performance improvement is available to everyone.
The amount of speedup to be expected depends on the operations used by
the program; simple low-level operations such as data transmission,
byte extraction, integer arithmetic, and simple branching can expect
to benefit the most. Function calling, and operations which already
spend most of their time in microcode, such as <code>ASSQ</code>, will
benefit the least.  In the best case one can achieve a factor of about
20. In the worst case, maybe no speedup at all.</p>

<p>Since the amount of control memory is limited, only a small number
of microcompiled functions can be loaded in at one time. This means
that programs have to be characterized by spending most of their time
in a small inner kernel of functions in order to benefit from
microcompilation; this is probably true of most programs.  There will
be fairly hairy metering facilities for identifying such critical
functions.</p>

<p>We do not yet have a microcompiler, but a prototype of one was
written and heavily used as part of the Lisp machine simulator.  It
compiles for the PDP-10 rather than CONS, but uses similar techniques
and a similar interface to the built-in microcode.</p>

<p>In all three forms of program, the flexibility of function calling
is augmented with generalized LAMBDA-lists.  In order to provide a
more general and flexible scheme to replace EXPRs vs. FEXPRs vs.
LEXPRs, a syntax borrowed from Muddle and Conniver is used in LAMBDA
lists. In the general case, there are an arbitrary number of REQUIRED
parameters, followed by an arbitrary number of OPTIONAL parameters,
possibly followed by one REST parameter. When a function is APPLIED to
its arguments, first of all the required formal parameters are paired
off with arguments; if there are fewer arguments than required
parameters, an error condition is caused. Then, any remaining
arguments are paired off with the optional parameters; if there are
more optional parameters than arguments remaining, then the rest of
the optional parameters are initialized in a user-specified manner.
The REST parameter is bound to a list, possibly NIL, of all arguments
remaining after all OPTIONAL parameters are bound.  To avoid CONSing,
this list is actually stored on the pdl; this means that you have to
be careful how you use it, unfortunately.  It is also possible to
control which arguments are evaluated and which are quoted.</p>

<p>Normally, such a complicated calling sequence would entail an
unacceptable amount of overhead.  Because this is all implemented by
microcode, and because the simple, common cases are special-cased, we
can provide these advanced features and still retain the efficiency
needed in a practical system.</p>

<p>We will now discuss some of the issues in the design of the
macrocode instruction set. Each macroinstruction is 16 bits long; they
are stored two per word. The instructions work in a stack-oriented
machine. The stack is formatted into frames; each frame contains a
bunch of arguments, a bunch of local variable value slots, a push-down
stack for intermediate results, and a header which gives the function
which owns the frame, links this frame to previous frames, remembers
the program counter and flags when this frame is not executing, and
may contain "additional information" used for certain esoteric
purposes. Originally this was intended to be a spaghetti stack, but
the invention of closures and stack-groups (see the control-structure
section), combined with the extreme complexity of spaghetti stacks,
made us decide to use a single linear stack. The current frame is
always held in the pdl buffer, so accesses to arguments and local
variables do not require memory references, and do not have to make
checks related to the garbage collector, which improves performance.
Usually several other frames will also be in the pdl buffer.</p>

<p>The macro instruction set is bit-compact. the stack organization
and Lisp's division of programs into small, separate functions means
that address fields can be small. The use of tagged data types,
powerful generic operations, and easily-called microcoded functions
makes a single 16-bit macro instruction do the work of several
instructions on a conventional machine such as a PDP-10.</p>

<p>The primitive operations which are the instructions which the
compiler generates are higher-level than the instructions of a
conventional machine.  They all do data type checks; this provides
more run-time error checking than in Maclisp, which increases
reliability. But it also eliminates much of the need to make
declarations in order to get efficient code. Since a data type check
is being made, the "primitive" operations can dynamically decide which
specific routine is to be called. This means that they are all
"generic", that is, they work for all data types where they make
sense.</p>

<p>The operations which are regarded as most important, and hence are
easiest for macrocode to do, are data transmission, function calling,
conditional testing, and simple operations on primitive types, that
is, <code>CAR</code>, <code>CDR</code>, <code>CADR</code>,
<code>CDDR</code>, <code>RPLACA</code>, and <code>RPLACD</code>, plus
the usual arithmetic operations and comparisons.  More complex
operations are generally done by "miscellaneous" instructions, which
call microcoded subroutines, passing arguments on the
temporary-results stack.</p>

<p>There are three main kinds of addressing in macrocode.  First,
there is implicit addressing of the top of the stack. This is the
usual way that operands get from one instruction to the next.</p>

<p>Second, there is the source field (this is sometimes used to store
results, but I will call it a source anyway). The source can address
any of the following: Up to 64 arguments to the current function. Up
to 64 local variables of the current function. The last result, popped
off the stack. One of several commonly-used constants (e.g.
<code>NIL</code>) stored in a system-wide constants area.  A constant
stared in the FEF of this function. A value cell or a function cell of
a symbol, referenced by means of an invisible pointer in the FEF; this
mode is used to reference special variables and to call other
functions.</p>

<p>Third, there is the destination field, which specifies what to do
with the result of the instruction. The possibilities are: Ignore it,
except set the indicators used by conditional branches. Push it on the
stack.  Pass it as an argument. Return it as the value of this
function. Cons up a list.</p>

<p>There are five types of macroinstructions, which will be described.
First, there are the data transmission instructions, which take the
source and <code>MOVE</code> it to the destination, optionally taking
<code>CAR</code>, <code>CDR</code>, <code>CAAR</code>,
<code>CADR</code>, <code>CDAR</code>, or <code>CDDR</code> in the
process.  Because of the powerful operations that can be specified in
the destination, these instructions also serve as argument-passing,
function-exiting, and list-making instructions.</p>

<p>Next we have the function calling instructions. The simpler of the
two is <code>CALL0</code>, call with no arguments. It calls the
function indicated by its source, and when that function returns, the
result is stored in the destination.  The microcode takes care of
identifying what type of function is being called, invoking it in the
appropriate way, and saving the state of the current function, It
traps to the interpreter if the called function is not compiled.</p>

<p>The more complex function call occurs when there are arguments. to
be passed.  The way it works is as follows. First, a <code>CALL</code>
instruction is executed.  The source operand is the function to be
called. The beginnings of a new stack frame are constructed at the end
of the current frame, and the function to be called is remembered. The
destination of the <code>CALL</code> instruction specifies where the
result of the function will be placed, and it is saved for later use
when the function returns.  Next, instructions are executed to compute
the arguments and store them into the destination
<code>NEXT-ARGUMENT</code>. This causes them to be added to the new
stack frame. When the last argument is computed, it is stored into the
destination <code>LAST-ARGUMENT</code>, which stores it in the new
stack frame and then activates the call. The function to be called is
analyzed, and the arguments are bound to the formal parameters
(usually the arguments are already in the correct slots of the new
stack frame).  Because the computation of the arguments is introduced
by a <code>CALL</code> instruction, it is easy to find out where the
arguments are and how many there are.  The new stack frame becomes
current and that function begins execution. When it returns, the saved
destination of the <code>CALL</code> instruction is retrieved and the
result is stored. Note that by using a destination of
<code>NEXT-ARGUMENT</code> or <code>LAST-ARGUMENT</code> function
calls may be nested.  By using a destination of <code>RETURN</code>
the result of one function may become the result of its caller.</p>

<p>The third class of macro instructions consists of a number of
common operations on primitive data types. These instructions do not
have an explicit destination, in order to save bits, but implicitly
push their result (if any) onto the stack.  This sometimes
necessitates the generation of an extra <code>MOVE</code> instruction
to put the result where it was really wanted. These instructions
include; Operations to store results from the pdl into the "source".
The basic arithmetic and bitwise boolean operations.  Comparison
operations, including <code>EQ</code> and arithmetic comparison, which
set the indicators for use by conditional branches.  Instructions
which set the "source" operand to <code>NIL</code> or zero.  Iteration
instructions which change the "source" operand using <code>CDR</code>,
<code>CDDR</code>, <code>1+</code>, or <code>1-</code> (add or
subtract one). Binding instructions which lambda-bind the "source"
operand, then optionally set it to <code>NIL</code> or to a value
popped off the stack.  And, finally, an instruction to push its
effective address on the stack, as a locative pointer.</p>

<p>The fourth class of macro instructions are the branches, which
serve mainly for compiling <code>COND</code>. Branches contain a
self-relative address which is transferred to if a specified condition
is satisfied. There are two indicators, which tell if the last result
was <code>NIL</code>, and if it was an atom, and the state of these
indicators can be branched on; there is also an unconditional branch,
of course.  For branches more then 256 half-words away, there is a
double-length long-branch instruction.  An interesting fact is that
there are not really any indicators; it turns out to be faster just to
save the last result in its entirety, and compare it against
<code>NIL</code> or whatever when that is needed by a branch
instruction.  It only has to be saved from one instruction to the
immediately following one.</p>

<p>The fifth class of macro instructions is the "miscellaneous
function."  This selects one of 512 microcoded functions to be called,
with arguments taken from results previously pushed on the stack.  A
destination is specified to receive the result of the function. In
addition to commonly-used functions such as <code>GET</code>,
<code>CONS</code>, <code>CDDDDR</code>, <code>REMAINDER</code>, and
<code>ASSQ</code>, miscellaneous functions include sub-primitives
(discussed above), and instructions which are not as commonly used as
the first four classes, including operations such as array-accessing,
consing up lists, un-lambda-binding, special funny types of function
calling, etc.</p>

<p>The way consing up of lists works is that one first does a
miscellaneous function saying "make a list <var>N</var> long".  One
then executes <var>N</var> instructions with destination
<code>NEXT-LIST</code> to supply the elements of the list.  After the
<code>N</code>th such instruction, the list-object magically appears
on the top of the stack.  This saves having to make a call to the
function <code>LIST</code> with a variable number of arguments.</p>

<p>Another type of "instruction set" used with macrocode is the
Argument Description List, which is executed by a different microcoded
interpreter at the time a function is entered. The ADL contains one
entry for each argument which the function expects to be passed, and
for each auxiliary variable.  It contains all relevant information
about the argument: whether it is required, optional, or rest, how to
initialize it if it is not provided, whether it is local or special,
datatype checking information, and so on.  Sometimes the ADL can be
dispensed with if the "fast argument option" can be used instead; this
helps save time and memory for small, simple functions.  The
fast-argument option is used when the optional arguments and local
variables are all to be initialized to <code>NIL</code>, there are not
too many of them, there is no data-type checking, and the usage of
special variables is not too complicated.  The selection of the
fast-argument option, if appropriate, is automatically made by the
system, so the user need not be concerned with it. The details can be
found in the FORMAT document.</p>

<h2>CONTROL STRUCTURES:</h2>

<h4>Function calling. </h4>

<p>Function calling is, of course, the
basic main control structure in Lisp. As mentioned above, Lisp machine
function calling is made fast through the use of microcode and
augmented with optional arguments, rest arguments, multiple return
values, and optional type-checking of arguments.</p>

<h4><code>CATCH</code> and <code>THROW</code>.</h4>

<p><code>CATCH</code> and <code>THROW</code> are a Maclisp control
structure which will be mentioned here since they may be new to some
people.  <code>CATCH</code> is a way of marking a particular point in
the stack of recursive function invocations. <code>THROW</code> causes
control to be unwound to the matching <code>CATCH</code>,
automatically returning through the intervening function calls. They
are used mainly for handling errors and unusual conditions. They are
also useful for getting out of a hairy piece of code when it has
discovered what value it wants to return; this applies particularly to
nested loops.</p>

<h4>Closures.</h4>

<p>The LISP machine contains a data-type called
"closure" which is used to implement "full funarging". By turning a
function into a closure, it.  becomes possible to pass it as an
argument with no worry about naming conflicts, and to return it as a
value with exactly the minimum necessary amount of binding environment
being retained, solving the classical "funarg problem". Closures are
implemented in such a way that when they are not used the highly
speed- and storage-efficient shallow binding variable scheme operates
at full efficiency, and when they are used things are slowed down only
slightly. The way one creates a closure is with a form such as: 
</p><pre>         (CLOSURE '(FOO-PARM FOO-STATE)
                   (FUNCTION FOO-BAR))
</pre>
The function could also be written directly in place as a
LAMBDA-expression, instead of referring to the externally defined
<code>FOO-BAR</code>.  The variables <code>FOO-PARAM</code> and
<code>FOO-STATE</code> are those variables which are used free by
<code>FOO-BAR</code> and are intended to be "closed". That is, these
are the variables whose binding environment is to be fixed to that in
effect at the time the closure is created.  The explicit declaration
of which variables are to be closed allows the implementation to have
high efficiency, since it does not need to save the whole
variable-binding environment, almost all of which is useless. It also
allows the programmer to explicitly choose for each variable whether
it is to be dynamically bound (at the point of call) or statically
bound (at the point of creation of the closure), a choice which is not
conveniently available in other languages.  In addition the program is
clearer because the intended effect of the closure is made manifest by
listing the variables to be affected.<p></p>

<p>Here is an example, in which the closure feature is used to solve a
problem presented in "LAMBDA - The Ultimate Imperative" [LAMBDA]. The
problem is to write a function called
<code>GENERATE-SQRT-OF-GIVEN-EXTRA-TOLERANCE</code>, which is to take
one argument, which is the factor by which the tolerance is to be
increased, and return a function which takes square roots with that
much more tolerance than usual, whatever "usual" is later defined to
be. You are given a function <code>SQRT</code> which makes a free
reference to <code>EPSILON</code>, which is the tolerance it demands
of the trial solution.  The reason this example presents difficulties
to various languages is that the variable <code>EPSILON</code> must be
bound at the point of call (i.e. dynamically scoped), while the
variable <code>FACTOR</code> must be bound at the point of creation of
the function (i.e. lexically scoped). Thus the programmer must have
explicit control over how the variables are bound.  
</p><pre> 
(DEFUN GENERATE-SQRT-OF-GIVEN-EXTRA-TOLERANCE (FACTOR)
     (CLOSURE '(FACTOR)
               (FUNCTION
                 (LAMBDA (X)
                   ((LAMBDA (EPSILON) (SQRT X))
                    (* EPSILON FACTOR))))))
</pre>
The function, when called, rebinds <code>EPSILON</code> to
<code>FACTOR</code> times its current value, then calls
<code>SQRT</code>. The value of <code>FACTOR</code> used is that in
effect when the closure was created, i.e. the argument to
<code>GENERATE-SQRT-0F-GIVEN-EXTRA-TOLERANCE</code>.  <p></p>

<p>The way closures are implemented is as follows. For each variable
to be closed an "external value cell" is created, which is a CONSed up
free-storage cell which contains the variable's value when it is at
that level of binding.  Because this cell is <code>CONS</code>ed up,
it can be retained as long as necessary, just like any other data, and
unlike cells in a stack. Because it is a cell, if the variable is
<code>SETQ</code>ed the new value is seen by all the closures that
should see it.  The association between the symbol which is the name
of the variable and this value cell is of the shallow-binding type,
for efficiency; an invisible pointer (see the storage organization
section) in the normal (internal) value cell supplies the connection,
eliminating the overhead of searching stack frames or a-lists.  If at
the time the closure is created an external value cell already exists
for a variable, that one is used instead of creating a new one, thus
all closures at the same "level of binding" use the same value cell,
which is the desired semantics.</p>

<p>The <code>CLOSURE</code> function returns an object of type
<code>DTP-CLOSURE</code>, which contains the function to be called
and, for each variable closed over, locative pointers to its internal
and external value cells.</p>

<p>When a closure is invoked as a function, the variables mentioned in
the closure are bound to invisible pointers to their external value
cells; this puts these variables into the proper binding environment.
The function contained in the closure is then invoked in the normal
way. When the variables happen to be referred to, the invisible
pointers are automatically followed to the external value cells.  If
one of the closed variables is then bound by some other function, the
external value cell pointer is saved away on the binding stack, like
any saved variable value, and the variable reverts to normal nonclosed
status.  When the closed function returns, the bindings of the closed
variables are restored just like any other variables bound by the
function.</p>

<p>Note the economy of mechanism. Almost all of the system is
completely unaffected by and unaware of the existence of closures; the
invisible pointer mechanism takes care of things.  The retainable
binding environments are allocated through the standard
<code>CONS</code> operation.  The switching of variables between
normal and "closed" status is done through the standard binding
operation.  The operations used by a closed function to access the
closed variables are the same as those used to access ordinary
variables; closures are called in the same way as ordinary functions.
Closures work just as well in the interpreter as in the compiler. An
important thing to note is the minimality of <code>CONS</code>ing in
closures. When a closure is created, some <code>CONS</code>ing is
done; external value cells and the closure-object itself must be
created, but there is no extra "overhead".  When a closure is called,
no <code>CONS</code>ing happens.</p>

<p>One thing to note is that in the compiler closed variables have to
be declared "special".  This is a general feature of the Maclisp and
Lisp machine compilers, that by default variables are local, which
means that they are lexically bound, only available to the function in
which they are bound, and implemented not with atomic symbols, but
simply as slots in the stack. Variables that are declared special are
implemented with shallow-bound atomic symbols, identical to variables
in the interpreter, and have available either dynamic binding or
closure binding. They are somewhat less efficient since it takes two
memory references to access them and several to bind them.</p>

<h4>Stack groups.</h4> 

<p>The stack group is a type of Lisp object useful for implementation
of certain advanced control structures such as coroutines,
asynchronous processes, and generators. A stack group is similar to a
process (or fork or job or task or control-point) in a time-sharing
system; it contains such state information as the "regular" and
"special" (binding) PDLs and various internal registers.  At all times
there is one stack group running on the machine.</p>

<p>Control may be passed between stack groups in several ways (not all
of which exist yet on our prototype machine). A stack-group may be
called like a function; when it wants to return it can do a
<code>%STACK-GROUP-RETURN</code> which is different from an ordinary
function return in that the state of the stack group remains
unchanged; the next time it is called it picks up from where it left
off.  This is good for generator-like applications; each time
<code>%STACK-GROUP-RETURN</code> is done, a value is emitted from the
generator, and as a side-effect execution is suspended until the next
time the generator is called.  <code>%STACK-GROUP-RETURN</code> is
analogous to the <code>ADIEU</code> construct in CONNIVER.</p>

<p>Control can simply be passed explicitly from one stack group to
another, coroutine-style.  Alternatively, there can be a scheduler
stack-group which invokes other stack groups when their requested
scheduling conditions are satisfied.</p>

<p>Interrupts cause control of the machine to be transferred to an
interrupt-handler stack group. Essentially this is a forced stack
group call like those calls described above. Similarly, when the
microcode detects an error the current stack group is suspended and
control is passed to an error-handling stack group.  The state of the
stack group that got the error is left exactly as it was when the
error occurred, undisturbed by any error-handling operations.  This
facilitates error analysis and recovery.</p>

<p>When the machine is started, an "initial" stack group becomes the
current stack group, and is forced to call the first function of
Lisp.</p>

<p>Note that the same scheduler-driven stack-group switching mechanism
can be used both for user programs which want to do parallel
computations, and for system programming purposes such as the handling
of network servers and peripheral handlers.</p>

<p>Each stack group has a call-state and a calling-stack-group
variable, which are used in maintaining the relations between stack
groups.  A stack group also has some option flags controlling whether
the system tries to keep different stack groups' binding environments
distinct by undoing the special variable bindings of the stack group
being left and redoing the bindings of the stack group being
entered.</p>

<p>Stack groups are created with the function
<code>MAKE-STACK-GROUP</code>, which takes one main argument, the
"name" of the stack group.  This is used only for debugging, and can
be any mnemonic symbol. It returns the stack group, i.e., a Lisp
object with data type <code>DTP-STACK-GROUP</code>. Optionally the
sizes of the pdls may be specified.</p>

<p>The function <code>STACK-GROUP-PRESET</code> is used to initialize
the state of a stack group: the first argument is the stack group, the
second is a function to be called when the stack group is invoked, and
the rest are arguments to that function.  Both PDLs are made empty.
The stack group is set to the <code>AWAITING-INITIAL-CALL</code>
state. When it is activated, the specified function will find that it
has been called with the specified arguments.  If it should return in
the normal way, (i.e. the stack group "returns off the top", the stack
group will enter a "used up" state and control will revert to the
calling stack group.  Normally, the specified function will use
<code>%STACK-GROUP-RETURN</code> several times; otherwise it might as
well have been called directly rather than in a stack group.</p>

<p>One important difference between stack groups and other means
proposed to implement similar features is that the stack group scheme
involves no loss of efficiency in normal computation.  In fact, the
compiler, the interpreter, and even the runtime function-calling
mechanism are completely unaware of the existence of stack groups.</p>

<h2>STORAGE ORGANIZATION:</h2>

<h4>Incremental Garbage Collection.</h4>

<p>The Lisp machine will use a
real-time, incremental, compacting garbage collector.  Real-time means
that <code>CONS</code> (or related functions) never delay Lisp
execution for more than a small, bounded amount of time.</p>

<p>This is very important in a machine with a large address space,
where a traditional garbage collection could bring everything to a
halt for several minutes.  The garbage collector is incremental, i.e.
garbage collection is interleaved with execution of the user's
program; every time you call <code>CONS</code> the garbage collection
proceeds for a few steps.  Copying can also be triggered by a memory
reference which fetches a pointer to data which has not yet been
copied.  The garbage collector compactifies in order to improve the
paging characteristics.</p>

<p>The basic algorithm is described in a paper by Henry Baker [GC]. We
have not implemented it yet, but design is proceeding and most of the
necessary changes to the microcode have already been made.  It is much
simpler than previous methods of incremental Garbage collection in
that only one process is needed; this avoids interlocking and
synchronization problems, which are often very difficult to debug.</p>

<h4>Areas.</h4>

<p>Storage in the Lisp machine is divided into "areas."  Each area
contains related objects, of any type.  Since unlike PDP-10 Lisps we
do not encode the data type in the address, we are free to use the
address to encode the area.  Areas are intended to give the user
control over the paging behavior of his program, among other things.
By putting related data together, locality can be greatly increased.
Whenever a new object is created, for instance with <code>CONS</code>,
the area to be used can optionally be specified.  There is a default
Working Storage area which collects those objects which the user has
not chosen to control explicitly.</p>

<p>Areas also give the user a handle to control the Garbage collector.
Some areas can be declared to be "static", which means that they
change slowly and the garbage collector should not attempt to reclaim
any space in them.  This can eliminate a lot of useless copying. All
pointers out of a static area can be collected into an "exit vector",
eliminating any need for the garbage collector to look at that area.
As an important example, an English-language dictionary can be kept
inside the Lisp without adversely affecting the speed of garbage
collection.  A "static" area can be explicitly garbage-collected at
infrequent intervals when it is believed that that might be
worthwhile.</p>

<p>Each area can potentially have a different storage discipline, a
different paging algorithm, and even a different data representation.
The microcode will dispatch on an attribute of the area at the
appropriate times.  The structure of the machine makes the performance
cost of these features negligible; information about areas is stored
in extra bits in the memory mapping hardware where it can be quickly
dispatched on by the microcode.  These dispatches usually have to be
done anyway to make the garbage collector work, and to implement
invisible pointers.</p>

<h4>Invisible Pointer.</h4>

<p>An invisible pointer is similar to an
indirect address word on a conventional computer except the
indirection is specified in the data instead of in the instruction. A
reference to a memory location containing an invisible pointer is
automatically altered to use the location pointed to by the invisible
pointer. The term "invisible" refers to the fact that the presence of
such pointers is not visible to most of the system, since they are
handled by the lowest-level memory-referencing operations.  The
invisible pointer feature does not slow anything down too much,
because it is part of the data type checking that is done anyway (this
is one of the benefits of a tagged architecture). A number of advanced
features of the Lisp machine depend upon invisible pointers for their
efficient implementation.</p>

<p>Closures use invisible pointers to connect internal value cells to
external value cells. This allows the variable binding scheme to be
altered from normal shallow binding to allocated-value-cell shallow
binding when closures are being used, without altering the normal
operation of the machine when closures are not being used.  At the
same time the slow-down when closures are used amounts to only 2
microseconds per closed-variable reference, the time needed to detect
and follow the invisible pointer.</p>

<p>Invisible pointers are necessary to the operation of the cdr-coded
compressed list scheme.  If an <code>RPLACD</code> is done to a
compressed list, the list can no longer be represented in the
compressed form. It is necessary to allocate a full 2-word cons node
and use that in its place. But, it is also necessary to preserve the
identity (with respect to <code>EQ</code>) of the list. This is done
by storing an invisible pointer in the original location of the
compressed list, pointing to the uncompressed copy.  Then the list is
still represented by its original location, preserving
<code>EQ</code>-ness, but the <code>CAR</code> and <code>CDR</code>
operations follow the invisible pointer to the new location and find
the proper car and cdr.</p>

<p>This is a special case of the more general use of invisible
pointers for "forwarding" references from an old representation of an
object to a new one.  For instance, there is a function to increase
the size of an array.  If it cannot do it in place, it makes a new
copy and leaves behind an invisible pointer.</p>

<p>The exit-vector feature uses invisible pointers. One may set up an
area to have the property that all references from inside that area to
objects in other areas are collected into a single exit-vector. A
location which would normally contain such a reference instead
contains an invisible pointer to the appropriate slot in the exit
vector. Operations on this area all work as before, except for a
slight slow-down caused by the invisible pointer following.  It is
also desirable to have automatic checking to prevent the creation of
new outside references; when an attempt is made to store an outside
object into this area execution can trap to a routine which creates a
new exit vector entry if necessary and stores an invisible pointer
instead. The reason for exit vectors is to speed up garbage collection
by eliminating the need to swap in all of the pages of the area in
order to find and relocate all its references to outside objects.</p>

<p>The macrocode instruction set relies on invisible pointers in order
to access the value cells of "special" (non-local) variables and the
function cells of functions to be called.</p>

<p>Certain system variables stored in the microcode scratchpad memory
are made available to Lisp programs by linking the value calls of
appropriately-named Lisp symbols to the scratchpad memory locations
with invisible pointers.  This makes it possible not only to read and
write these variables, but also to lambda-bind them.  In a similar
fashion, invisible pointers could be used to link two symbols' value
cells together, in the fashion of Microplanner but with much greater
efficiency.</p>

<h2>THE EDITOR:</h2>

<p>The Lisp machine system includes an advanced real-time display
oriented editor, which is written completely in Lisp. The design of
this editor drew heavily on our experience with the EMACS editor (and
its predecessors) on the PDP-10. The high-speed display and fast
response time of the Lisp machine are crucial to the success of the
editor.</p>

<p>The TV display is used to show a section of the text buffer
currently being edited.  When the user types a normal printing
character on the keyboard, that character is inserted into his buffer,
and the display of the buffer is updated; you see the text as you type
it in. When using an editor, most of the user's time is spent in
typing in text; therefore, this is made as easy as possible.  Editing
operations other than the insertion of single characters are invoked
by control-keys, i.e., by depressing the <code>CONTROL</code> and/or
<code>META</code> shift keys, along with a single character. For
example, the command to move the current location for typein in the
buffer (the "point") backward is <code>Control-B</code> (B is mnemonic
for Backward), the command to move to the next line is
<code>Control-N</code>.  There are many more advanced commands, which
know how to interpret the text as words or as the printed
representation of Lisp data structure; <code>Meta-F</code> moves
forward over an English word, and <code>Control-Meta-F</code> moves
forward over a Lisp expression (an atom or a list).</p>

<p>The real-time display-oriented type of editor is much easier to use
than traditional text editors, because you can always see exactly what
you are doing.  A non user can sit right down and type in text.
However, this does not mean that there can be no sophisticated
commands and macros. Very powerful operations are provided in the Lisp
machine editor. Self-documentation features exist to allow the user to
ask what a particular key does before trying it, and to ask what keys
contain a given word in their description. Users can write additional
commands, in Lisp, and add them to the editor's command tables.</p>

<p>The editor knows how much a line should be indented in a Lisp
program in order to reflect the level of syntactic nesting. When
typing in Lisp code, one uses the linefeed key after typing in a line
to move to the next line and automatically indent it by the right
amount. This serves the additional purpose of instantly pointing out
errors in numbers of parentheses.</p>

<p>The editor can be used as a front end to the Lisp top level loop.
This provides what can be thought of as very sophisticated rubout
processing.  When the user is satisfied that the form as typed is
correct, he can activate it, allowing Lisp to read in the form and
evaluate it. When Lisp prints out the result, it is inserted into the
buffer at the right place.  Single commands are available to fetch
earlier inputs, for possible editing and reactivation.</p>

<p>In addition to commands from the keyboard, the mouse can be used to
point to parts of the buffer, and to give simple editing commands. The
use of mice for text editing was originated at SRI, and has been
refined and extended at XEROX-PARC.</p>

<p>The character-string representation of each function in a program
being worked on is stored in its own editor buffer. One normally
modifies functions by editing the character-string form, than typing a
single-character command to read it into Lisp, replacing the old
function.  Compilation can optionally be requested.  The advantage of
operating on the character form, rather than directly on the list
structure, is that comments and the user's chosen formatting of the
code are preserved; in addition, the editor is easier to use because
it operates on what you see on the display. There are commands to
store sets of buffers into files, and to get them back again</p>

<p>The editor has the capability to edit and display text in multiple
fonts, and many other features too numerous to mention here.</p>

<h2>CURRENT STATUS (August 1977)</h2>

<p>The original prototype CONS Machine was designed and built somewhat
more than two years ago.  It had no memory and no I/0 capability, and
remained pretty much on the back burner while software was developed
with a simulator on the PDP-10 (the simulator executed the Lisp
machine macro instruction set, a function now performed by CONS
microcode.)  Microprogramming got under way a little over a year ago,
and in the beginning of 1977 the machine got memory, a disk, and a
terminal.</p>

<p>We now have an almost-complete system running on the prototype
machine.  The major remaining "holes" are the lack of a garbage
collector and the presence of only the most primitive error handling.
Also, floating-point and big-integer numbers and microcompilation have
been put off until the next machine.  The system includes almost all
the functions of Maclisp, and quite a few new ones.  The machine is
able to page off of its disk, accept input from the keyboard and the
mouse, display on the TV, and do I/O to files on the PDP-10. The
display editor is completely working, and the compiler runs on the
machine, so the system is quite usable for typing in, editing,
compiling, and debugging Lisp functions.</p>

<p>As a demonstration of the system, and a test of its capabilities,
two large programs have been brought over from the PDP-10. William
Woods's LUNAR English-language data-base query system was converted
from InterLisp to Maclisp, thence to Lisp machine Lisp. On the Lisp
machine it runs approximately 3 times as fast as in Maclisp on the
KA-10, which in turn is 2 to 4 times as fast as in InterLisp.  Note
that the Lisp machine time is elapsed real time, while the PDP-10
times are virtual run times is given by the operating system and do
not include the delays due to timesharing.</p>

<p>Most of the Macsyma symbolic algebraic system his been converted to
the Lisp machine; nearly all the source files were simply compiled
without any modifications.  Most of Macsyma works except for some
things that require bignums.  The preliminary speed is the same as on
the KA-10, but a number of things have not been optimally converted.
(This speed measurement is, again, elapsed time on the Lisp machine
version versus reported run time on the KA-10 time sharing system.
Thus, paging and scheduling overhead in the KA-10 case are not counted
in this measurement.)</p>

<p>LUNAR (including the dictionary) and Macsyma can reside together in
the Lisp machine with plenty of room left over; either program alone
will not entirely fit in a PDP-10 address space.</p>

<p>The CONS machine is currently being redesigned and a new machine
will be built soon, replacing our present prototype.  The new machine
will have larger sizes for certain internal memories, will incorporate
newer technology, will have greatly improved packaging, and will be
faster.  It will fit entirely in one cabinet and will be designed for
ease of construction and servicing.  In late 1977 and early 1978 we
plan to build seven additional machines and install them at the MIT AI
Lab.  During the fall of 1977 we plan to finish the software, bringing
it to a point where users can be put on the system. User experience
with the Lisp machine during 1978 should result in improvement and
cleaning up of the software and documentation, and should give us a
good idea of the real performance to be expected from the machine. At
that time we will be able to start thinking about ways to make Lisp
machines available to the outside world.</p>

<h2>REFERENCES:</h2>
<div style="line-height: 20pt">
CONS: Steele, Guy L. "Cons", not yet published. This is a revision
                              Working Paper 80, CONS by Tom Knight<br><br>

GC: Baker, Henry,  "List Processing in Real Time on a Serial Computer", 
                 Working Paper 139 <br><br>

LAMBDA:  Steele, Guy L. "LAMBDA - The Ultimate Imperative, Artificial
              Intelligence Memo 353<br><br>

MOUSE:  See extensive publications by Englebart and group at SRI.<br><br>
</div>


</body></html>
<!-- Edwin Variables: -->
<!-- Mode: text -->
<!-- End: -->
